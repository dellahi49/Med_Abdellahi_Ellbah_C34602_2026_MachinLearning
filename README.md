#  Projet Machine Learning ‚Äì R√©gression Lin√©aire & R√©gression Logistique

##  Objectif du projet
Ce projet vise √† appliquer des **algorithmes de Machine Learning Supervis√© (MLS)** afin de r√©soudre :
- un probl√®me de **r√©gression** (pr√©diction d‚Äôune variable continue),
- un probl√®me de **classification** (pr√©diction d‚Äôune classe),

en suivant une **d√©marche m√©thodologique compl√®te** allant de l‚Äôexploration des donn√©es √† l‚Äô√©valuation des performances.

---

##  Algorithmes de Machine Learning utilis√©s

|------Partie------|----------Type--------|-----Algorithme--------|
|------------------|----------------------|-----------------------|
| Partie 1         | MLS ‚Äì R√©gression     | R√©gression Lin√©aire   |
| Partie 2         | MLS ‚Äì Classification | R√©gression Logistique |

```

## Contenu du projet

```
```
.
‚îú‚îÄ‚îÄ mini-projet-RLineair.ipynb  # Notebook Google Colab de  R√©gression Lin√©aire 
‚îú‚îÄ‚îÄ mini-projet-Rlogistique.ipynb  # Notebook Google Colab de R√©gression Logistique
‚îî‚îÄ‚îÄ README.md        # Documentation du projet
```




##  Partie 1 : R√©gression Lin√©aire  
### Dataset : Medical Insurance Cost

###  Description du jeu de donn√©es

Ce jeu de donn√©es contient des informations sur les **co√ªts d‚Äôassurance m√©dicale pour 1 338 individus**.  
Il inclut des variables **d√©mographiques** et **li√©es √† la sant√©**, telles que l‚Äô√¢ge, le sexe, l‚Äôindice de masse corporelle (IMC), le nombre d‚Äôenfants, le statut de fumeur et la r√©gion de r√©sidence aux √âtats-Unis.

La **variable cible** est `charges`, qui repr√©sente le **co√ªt de l‚Äôassurance m√©dicale factur√© √† l‚Äôindividu**.

Ce dataset est couramment utilis√© pour :
- la **mod√©lisation par r√©gression**,
- la **recherche en √©conomie de la sant√©**,
- l‚Äô**analyse de la tarification des assurances**,
- l‚Äô**enseignement du machine learning** et des techniques d‚Äôing√©nierie des caract√©ristiques.

####  Colonnes du dataset
- **age** : √¢ge du b√©n√©ficiaire principal (entier)
- **sex** : sexe du b√©n√©ficiaire (male, female)
- **bmi** : indice de masse corporelle (IMC), mesure de la masse grasse bas√©e sur la taille et le poids (r√©el)
- **children** : nombre d‚Äôenfants couverts par l‚Äôassurance sant√© (entier)
- **smoker** : statut de fumeur du b√©n√©ficiaire (yes, no)
- **region** : r√©gion de r√©sidence aux √âtats-Unis (northeast, northwest, southeast, southwest)
- **charges** : co√ªt de l‚Äôassurance m√©dicale factur√© au b√©n√©ficiaire (r√©el)

####  Utilisations potentielles
- Construire des **mod√®les pr√©dictifs** des co√ªts m√©dicaux
- √âtudier l‚Äôimpact du **tabagisme** et de l‚Äô**IMC** sur les d√©penses de sant√©
- Illustrer les concepts de **r√©gression lin√©aire** et de **feature engineering**
- Analyser les tendances li√©es √† l‚Äôaccessibilit√© des soins

---

###  √âtapes m√©thodologiques suivies

#### 1 Chargement et exploration des donn√©es
- Lecture du dataset
- Visualisation des premi√®res lignes
- Analyse des types de variables

#### 2 Pr√©traitement des donn√©es
- S√©paration des variables explicatives et de la variable cible
- Encodage des variables cat√©gorielles (One-Hot Encoding)
- V√©rification des valeurs manquantes

#### 3 Analyse exploratoire
- Calcul de la matrice de corr√©lation
- Visualisation via une **Heatmap**
- Analyse des relations entre variables

#### 4 Mod√©lisation math√©matique
Le mod√®le de r√©gression lin√©aire est d√©fini par :

\[
y = \beta_0 + \sum_{i=1}^{n} \beta_i x_i + \varepsilon
\]

#### 5 Entra√Ænement du mod√®le
- S√©paration des donn√©es en ensembles d‚Äôentra√Ænement et de test
- Utilisation de **LinearRegression** (scikit-learn)

#### 6 √âvaluation des performances
Les performances sont √©valu√©es √† l‚Äôaide de :
- **Mean Squared Error (MSE)**
- **Coefficient de d√©termination (R¬≤)**

#### 7 Interpr√©tation des r√©sultats
- Analyse de l‚Äôinfluence de chaque variable explicative
- Identification des facteurs ayant le plus d‚Äôimpact sur les co√ªts m√©dicaux

---

##  Partie 2 : R√©gression Logistique  


Ce projet illustre l‚Äôapplication de la **r√©gression logistique** sur le dataset **Iris** (disponible dans *scikit-learn*). Le probl√®me est transform√© en **classification binaire** :

* **Classe 1** : Iris *Setosa* (classe 0)
* **Classe 0** : Toutes les autres esp√®ces (*Versicolor* et *Virginica*)

L‚Äôobjectif est de pr√©dire la probabilit√© d‚Äôappartenance √† la classe *Setosa* √† partir des caract√©ristiques des fleurs.

---

---

##  Donn√©es

Le dataset **Iris** contient 150 observations avec 4 variables explicatives :

* Longueur du s√©pale (sepal length)
* Largeur du s√©pale (sepal width)
* Longueur du p√©tale (petal length)
* Largeur du p√©tale (petal width)

La variable cible originale comporte 3 classes. Elle est transform√©e en une **variable binaire** :

* `1` : Setosa
* `0` : Autres

---

## üõ† √âtapes m√©thodologiques suivies

### 1 Pr√©paration des donn√©es

* Chargement du dataset Iris depuis `sklearn.datasets`
* Conversion en DataFrame pour faciliter la manipulation
* Transformation de la variable cible en classification binaire

### 2 S√©paration des donn√©es

* Division en **jeu d‚Äôentra√Ænement (80%)** et **jeu de test (20%)**
* Objectif : entra√Æner le mod√®le sur une partie des donn√©es et √©valuer sa performance sur des donn√©es jamais vues

### 3 Normalisation

* Application de la **standardisation** (moyenne = 0, √©cart-type = 1)
* Permet d‚Äôam√©liorer la convergence et la performance du mod√®le de r√©gression logistique

### 4 Mod√©lisation : R√©gression Logistique

La r√©gression logistique estime la probabilit√© :

[ P(y = 1 | x) = \frac{1}{1 + e^{-z}} ]

avec :

[ z = w^T x + b ]

* `x` : vecteur des variables d‚Äôentr√©e
* `w` : poids du mod√®le
* `b` : biais

La fonction **sigmo√Øde** transforme la sortie en une probabilit√© comprise entre 0 et 1.

### 5 Pr√©diction

* Le mod√®le pr√©dit la classe (0 ou 1) pour chaque observation du jeu de test

### 6 √âvaluation du mod√®le

#### 7 Matrice de confusion

La matrice de confusion permet d‚Äôanalyser les erreurs de classification :

|            | Pr√©dit 0 | Pr√©dit 1 |
| ---------- | -------- | -------- |
| **R√©el 0** | TN       | FP       |
| **R√©el 1** | FN       | TP       |

* **TP (True Positives)** : Setosa correctement d√©tect√©es
* **TN (True Negatives)** : Autres classes correctement d√©tect√©es
* **FP (False Positives)** : Autres classes pr√©dites √† tort comme Setosa
* **FN (False Negatives)** : Setosa non d√©tect√©es

#### 8 M√©triques

* **Accuracy (Exactitude)**
  [ Accuracy = \frac{TP + TN}{TP + TN + FP + FN} ]

* **Precision (Pr√©cision)**
  [ Precision = \frac{TP}{TP + FP} ]

* **Recall (Rappel)**
  [ Recall = \frac{TP}{TP + FN} ]

Ces indicateurs permettent d‚Äô√©valuer la qualit√© globale du mod√®le ainsi que sa capacit√© √† d√©tecter correctement la classe positive (Setosa).

---

##  Ex√©cution

1. Ouvrir les notebooks dans **Google Colab**
2. Ex√©cuter les cellules dans l‚Äôordre :

   * Importation des biblioth√®ques
   * Chargement des donn√©es
   * Transformation en classification binaire
   * S√©paration des donn√©es
   * Normalisation
   * Entra√Ænement du mod√®le
   * Pr√©dictions
   * √âvaluation (matrice de confusion et m√©triques)

---

##  R√©sultats attendus

* Une **forte accuracy** indiquant une bonne performance globale
* Une **pr√©cision √©lev√©e** montrant que les pr√©dictions de Setosa sont fiables
* Un **recall √©lev√©** indiquant que la majorit√© des Setosa sont correctement identifi√©es

---

##  Outils et technologies utilis√©s
- Python 3
- Pandas, NumPy
- Matplotlib, Seaborn
- Scikit-learn
- Google Colab

---
Auteur : medabdellahi elbah


